{
  "nbformat": 4,
  "nbformat_minor": 0,
  "metadata": {
    "colab": {
      "private_outputs": true,
      "provenance": [],
      "authorship_tag": "ABX9TyPkjMrd2A6PqE0AZ0qpNMbh",
      "include_colab_link": true
    },
    "kernelspec": {
      "name": "python3",
      "display_name": "Python 3"
    },
    "language_info": {
      "name": "python"
    }
  },
  "cells": [
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "view-in-github",
        "colab_type": "text"
      },
      "source": [
        "<a href=\"https://colab.research.google.com/github/Ayman850/COMP257/blob/main/COMP257_Ass2_AymanYasin.ipynb\" target=\"_parent\"><img src=\"https://colab.research.google.com/assets/colab-badge.svg\" alt=\"Open In Colab\"/></a>"
      ]
    },
    {
      "cell_type": "markdown",
      "source": [
        "# Assignment2 - Question 1"
      ],
      "metadata": {
        "id": "izR20d9p74HJ"
      }
    },
    {
      "cell_type": "markdown",
      "source": [
        "## Imports"
      ],
      "metadata": {
        "id": "x4cMQAJx8bv1"
      }
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {
        "id": "4Xrf0oD9X-wp"
      },
      "outputs": [],
      "source": [
        "import matplotlib.pyplot as plt\n",
        "import numpy as np\n",
        "import pandas as pd\n",
        "\n",
        "from collections import Counter\n",
        "from sklearn.decomposition import PCA\n",
        "from sklearn.svm import SVC\n",
        "from sklearn.preprocessing import StandardScaler\n",
        "from sklearn.datasets import fetch_olivetti_faces\n",
        "from sklearn.model_selection import train_test_split, StratifiedKFold, cross_val_score\n",
        "from sklearn.metrics import accuracy_score, silhouette_score\n",
        "from sklearn.cluster import KMeans, DBSCAN"
      ]
    },
    {
      "cell_type": "markdown",
      "source": [
        "## 1. Retrieve and load Olivetti faces"
      ],
      "metadata": {
        "id": "-M05VgZP8jlY"
      }
    },
    {
      "cell_type": "code",
      "source": [
        "# Retrieve the Olivetti faces\n",
        "\n",
        "olivetti_faces = fetch_olivetti_faces()"
      ],
      "metadata": {
        "id": "RoSN55ZE588K"
      },
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "code",
      "source": [
        "#shape of the data\n",
        "X = olivetti_faces.data\n",
        "y = olivetti_faces.target\n",
        "images = olivetti_faces.images # 64x64 version of .data\n",
        "print(X.shape)\n",
        "print(y.shape)"
      ],
      "metadata": {
        "id": "rc8D-LcN8hAl"
      },
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "code",
      "source": [
        "# I Plot first 15 images\n",
        "fig, axes = plt.subplots(3, 5, figsize=(10, 5))\n",
        "\n",
        "for i, ax in enumerate(axes.ravel()):\n",
        "    ax.imshow(images[i], cmap='gray')\n",
        "    ax.set_title(f\"ID: {y[i]}\")\n",
        "    ax.axis('off')\n",
        "\n",
        "plt.tight_layout()\n",
        "plt.show()\n"
      ],
      "metadata": {
        "id": "wXMAORTd8nlV"
      },
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "code",
      "source": [
        "# From Reference [1]\n",
        "# I use the similar notebook which exploring this dataset and using the same function aswell\n",
        "\n",
        "def show_40_distinct_people(images, unique_ids):\n",
        "    #Creating 4X10 subplots in  18x9 figure size\n",
        "    fig, axarr=plt.subplots(nrows=4, ncols=10, figsize=(18, 9))\n",
        "    #For easy iteration flattened 4X10 subplots matrix to 40 array\n",
        "    axarr=axarr.flatten()\n",
        "\n",
        "    #iterating over user ids\n",
        "    for unique_id in unique_ids:\n",
        "        image_index=unique_id*10\n",
        "        axarr[unique_id].imshow(images[image_index], cmap='gray')\n",
        "        axarr[unique_id].set_xticks([])\n",
        "        axarr[unique_id].set_yticks([])\n",
        "        axarr[unique_id].set_title(\"face id:{}\".format(unique_id))\n",
        "    plt.suptitle(\"There are 40 distinct people in the dataset\")\n",
        "\n",
        "show_40_distinct_people(images, np.unique(y))\n",
        "\n",
        "#here now it ploting 1 image per face id, now we can see 40 diffrent faces, totall is 400, 10 images for 40 people"
      ],
      "metadata": {
        "id": "R6CMvCYTtMQJ"
      },
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "markdown",
      "source": [
        "## 2. Split train, val, test with stratified sampling"
      ],
      "metadata": {
        "id": "ldANAfki8PVm"
      }
    },
    {
      "cell_type": "code",
      "source": [
        "# I choose because for 1 subject there are 10 images\n",
        "\n",
        "# train has 8 images for 1 person, 1 images for validation, and 1 images for test\n",
        "'''\n",
        "Split is:\n",
        "80% train\n",
        "10% validation\n",
        "10% test\n",
        "to ensure we have a lot of data to train,\n",
        "and also still have some left for validation and testing\n",
        "'''\n",
        "# First split into train + validation (90%) and test (15%)\n",
        "X_train_val, X_test, y_train_val, y_test = train_test_split(\n",
        "    X, y, test_size=0.1, random_state=42, stratify=y)\n",
        "\n",
        "# Now split the train + validation set (90%) into train (80%) and validation (10%)\n",
        "# 10/90 = 0.1111\n",
        "X_train, X_val, y_train, y_val = train_test_split(\n",
        "    X_train_val, y_train_val, test_size=0.1111, random_state=42, stratify=y_train_val)  # I using stratify to make it balance"
      ],
      "metadata": {
        "id": "qFLKzu1w7yuI"
      },
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "code",
      "source": [
        "print(X_train.shape)\n",
        "print(X_val.shape)\n",
        "print(X_test.shape)"
      ],
      "metadata": {
        "id": "UJsshtq18kdj"
      },
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "code",
      "source": [
        "#Here is just to check what I said is correct\n",
        "# From Reference [1]\n",
        "y_frame=pd.DataFrame()\n",
        "y_frame['subject ids']=y_train\n",
        "y_frame.groupby(['subject ids']).size().plot.bar(figsize=(15,8),title=\"Number of Samples for Each Classes (TRAIN)\")"
      ],
      "metadata": {
        "id": "NbbVK_7k8osW"
      },
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "code",
      "source": [
        "# From Reference [1]\n",
        "y_frame=pd.DataFrame()\n",
        "y_frame['subject ids']=y_val\n",
        "y_frame.groupby(['subject ids']).size().plot.bar(figsize=(15,8),title=\"Number of Samples for Each Classes (VALIDATION)\")"
      ],
      "metadata": {
        "id": "hHbdRGGC8thz"
      },
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "code",
      "source": [
        "# From Reference [1]\n",
        "y_frame=pd.DataFrame()\n",
        "y_frame['subject ids']=y_test\n",
        "y_frame.groupby(['subject ids']).size().plot.bar(figsize=(15,8),title=\"Number of Samples for Each Classes (TEST)\")"
      ],
      "metadata": {
        "id": "OhFkpHAY82Rn"
      },
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "markdown",
      "source": [
        "## 3. Use k-fold CV, train a classifier, and evaluate on validation set"
      ],
      "metadata": {
        "id": "yND9knIl88yY"
      }
    },
    {
      "cell_type": "code",
      "source": [
        "# Use SVC with linear kernel\n",
        "clf = SVC(kernel='linear', random_state=42)\n",
        "\n",
        "# Instantiate StratifiedKFold\n",
        "# use n_splits=4 so the 8 training samples per class can be evenly distributed of 2 samples each\n",
        "kf = StratifiedKFold(n_splits=4, shuffle=True, random_state=42)\n",
        "\n",
        "# Run k-fold CV\n",
        "cv_scores = cross_val_score(clf, X_train, y_train, cv=kf, scoring='accuracy')\n",
        "\n",
        "# Print scores\n",
        "print(f\"Cross-validation accuracy scores: {cv_scores}\")\n",
        "print(f\"Mean cross-validation accuracy: {cv_scores.mean():.4f}\")\n",
        "\n",
        "# Train the classifier on the entire training set\n",
        "clf.fit(X_train, y_train)\n",
        "\n",
        "# Evaluate the classifier on the validation set\n",
        "y_val_pred = clf.predict(X_val)\n",
        "val_accuracy = accuracy_score(y_val, y_val_pred)\n",
        "\n",
        "print(f\"Validation set accuracy: {val_accuracy:.4f}\")"
      ],
      "metadata": {
        "id": "W7XMzRJj87wR"
      },
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "markdown",
      "source": [
        "## 4. Use K-Means to reduce the dimensionality of the set. Provide your rationale for the similarity measure used to perform the clustering. Use the silhouette score approach to choose the number of clusters."
      ],
      "metadata": {
        "id": "nk8q6XBH9GMD"
      }
    },
    {
      "cell_type": "code",
      "source": [
        "# Try different values of k from 20 to 150 bcz of 40 faces, where 20 is lower then 40 and 150 is high (reson the rest I mentioned on report)\n",
        "cluster_range = range(20, 150)\n",
        "silhouette_scores = []\n",
        "\n",
        "# Perform KMeans clustering and compute the silhouette score for each number of clusters\n",
        "for k in cluster_range:\n",
        "    kmeans = KMeans(n_clusters=k, random_state=42)\n",
        "    cluster_labels = kmeans.fit_predict(X)\n",
        "    silhouette_avg = silhouette_score(X, cluster_labels)\n",
        "    silhouette_scores.append(silhouette_avg)\n",
        "    print(f\"For k = {k}, the silhouette score is {silhouette_avg:.4f}\")\n",
        "\n",
        "# Plot silhouette scores to visualize the optimal number of clusters\n",
        "plt.figure(figsize=(8, 5))\n",
        "plt.plot(cluster_range, silhouette_scores, marker='o')\n",
        "plt.xlabel('Number of Clusters (k)')\n",
        "plt.ylabel('Silhouette Score')\n",
        "plt.title('Silhouette Score vs Number of Clusters')\n",
        "plt.grid(True)\n",
        "plt.show()\n",
        "\n",
        "# Choose the number of clusters with the highest silhouette score\n",
        "optimal_k = cluster_range[np.argmax(silhouette_scores)]\n",
        "print(f\"The optimal number of clusters is: {optimal_k}\")\n"
      ],
      "metadata": {
        "id": "h0r16gkM9HHp"
      },
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "code",
      "source": [
        "# here I used optimal_k to reduce\n",
        "optimal_k = 129\n",
        "# Reduce dimensionality to the optimal k found previously\n",
        "kmeans_optimal = KMeans(n_clusters=optimal_k, random_state=42)\n",
        "X_reduced = kmeans_optimal.fit_transform(X)"
      ],
      "metadata": {
        "id": "w90K1rY59N6l"
      },
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "code",
      "source": [
        "# here you can see the result for reduce\n",
        "print(X.shape)\n",
        "print(X_reduced.shape)"
      ],
      "metadata": {
        "id": "PPiquGyz9ROT"
      },
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "code",
      "source": [
        "# labels will become which cluster the data belongs to\n",
        "# instead of the person id\n",
        "y_reduced = kmeans_optimal.labels_\n",
        "print(y_reduced.shape)\n",
        "print(y_reduced)"
      ],
      "metadata": {
        "id": "X-DTqR4e9UTz"
      },
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "code",
      "source": [
        "y_frame=pd.DataFrame()\n",
        "y_frame['subject ids']=y_reduced\n",
        "y_frame.groupby(['subject ids']).size().plot.bar(figsize=(15,8),title=\"Number of Samples for Each Classes\")"
      ],
      "metadata": {
        "id": "y2qeKG6T9YZZ"
      },
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "markdown",
      "source": [
        "## 5. Use set from step 4 to train classifier as in step 3"
      ],
      "metadata": {
        "id": "emlcRCaE9dys"
      }
    },
    {
      "cell_type": "code",
      "source": [
        "X = X_reduced\n",
        "y = y_reduced\n",
        "\n",
        "print(X.shape)\n",
        "print(y.shape)"
      ],
      "metadata": {
        "id": "tsWewWrW9eoQ"
      },
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "code",
      "source": [
        "# First split into train + validation (90%) and test (15%)\n",
        "X_train_val, X_test, y_train_val, y_test = train_test_split(\n",
        "    X, y, test_size=0.1, random_state=42)\n",
        "\n",
        "# Now split the train + validation set (90%) into train (80%) and validation (10%)\n",
        "# 10/90 = 0.1111\n",
        "X_train, X_val, y_train, y_val = train_test_split(\n",
        "    X_train_val, y_train_val, test_size=0.1111, random_state=42)"
      ],
      "metadata": {
        "id": "GJiAC6o_9pFz"
      },
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "code",
      "source": [
        "# Use SVC with linear kernel\n",
        "clf = SVC(kernel='linear', random_state=42)\n",
        "\n",
        "# Instantiate StratifiedKFold\n",
        "kf = StratifiedKFold(n_splits=2, shuffle=True, random_state=42)\n",
        "\n",
        "# Run k-fold CV\n",
        "cv_scores = cross_val_score(clf, X_train, y_train, cv=kf, scoring='accuracy')\n",
        "\n",
        "# Print scores\n",
        "print(f\"Cross-validation accuracy scores: {cv_scores}\")\n",
        "print(f\"Mean cross-validation accuracy: {cv_scores.mean():.4f}\")\n",
        "\n",
        "# Train the classifier on the entire training set\n",
        "clf.fit(X_train, y_train)\n",
        "\n",
        "# Evaluate the classifier on the validation set\n",
        "y_val_pred = clf.predict(X_val)\n",
        "val_accuracy = accuracy_score(y_val, y_val_pred)\n",
        "\n",
        "print(f\"Validation set accuracy: {val_accuracy:.4f}\")"
      ],
      "metadata": {
        "id": "PXtH6zo_-GXT"
      },
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "markdown",
      "source": [
        "## 6. Apply DBSCAN to dataset. Preprocess images and convert into feature vectors. Use DBSCAN to group images based on density."
      ],
      "metadata": {
        "id": "Mk4CCMp3-Kro"
      }
    },
    {
      "cell_type": "markdown",
      "source": [
        "### a. Using K-Means like previous step"
      ],
      "metadata": {
        "id": "3kr_IRs0-OQ2"
      }
    },
    {
      "cell_type": "code",
      "source": [
        "# Reload data\n",
        "olivetti_faces = fetch_olivetti_faces()\n",
        "X = olivetti_faces.data\n",
        "y = olivetti_faces.target\n",
        "\n",
        "optimal_k = 125\n",
        "# Reduce dimensionality to the optimal k found previously\n",
        "kmeans_optimal = KMeans(n_clusters=optimal_k, random_state=42)\n",
        "X_reduced = kmeans_optimal.fit_transform(X)\n",
        "print(f\"Before: {X.shape}\")\n",
        "print(f\"After: {X_reduced.shape}\")"
      ],
      "metadata": {
        "id": "1ZYSYRk4-Ljr"
      },
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "code",
      "source": [
        "# use DBSCAN\n",
        "eps = 10\n",
        "min_samples = 4\n",
        "dbscan = DBSCAN(eps=eps, min_samples=min_samples, metric='euclidean')\n",
        "dbscan.fit(X_reduced)\n",
        "\n",
        "labels = dbscan.labels_\n",
        "n_clusters = len(set(labels)) - (1 if -1 in labels else 0)  # Subtract 1 for noise (-1 label)\n",
        "cluster_counts = Counter(labels)\n",
        "n_noise = list(labels).count(-1)\n",
        "\n",
        "print(f'eps: {eps}')\n",
        "print(f'min_samples: {min_samples}')\n",
        "print(f'Number of clusters: {n_clusters}')\n",
        "print(f'Number of noise points: {n_noise}')\n",
        "print(f\"Number of samples in each cluster:\")\n",
        "for label, count in sorted(cluster_counts.items()):\n",
        "    if label == -1:\n",
        "        print(f\"Noise (label -1): {count} samples\")\n",
        "    else:\n",
        "        print(f\"Cluster {label}: {count} samples\")\n",
        "if n_clusters > 1:\n",
        "    silhouette_avg = silhouette_score(X_reduced, labels)\n",
        "    print(f\"Silhouette Score: {silhouette_avg}\")"
      ],
      "metadata": {
        "id": "_ilyJSmdoc5S"
      },
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "markdown",
      "source": [
        "### b. Using PCA to reduce to 2D"
      ],
      "metadata": {
        "id": "DzIMUkl1oiyq"
      }
    },
    {
      "cell_type": "code",
      "source": [
        "# Reload data\n",
        "olivetti_faces = fetch_olivetti_faces()\n",
        "X = olivetti_faces.data\n",
        "y = olivetti_faces.target\n",
        "\n",
        "# Reduce dimensionality to 2D with PCA\n",
        "pca = PCA(n_components=2)\n",
        "X_reduced = pca.fit_transform(X)\n",
        "\n",
        "print(f\"Before: {X.shape}\")\n",
        "print(f\"After: {X_reduced.shape}\")"
      ],
      "metadata": {
        "id": "Dn4ETiuVojzz"
      },
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "code",
      "source": [
        "# use DBSCAN\n",
        "eps = 1.5\n",
        "min_samples = 4\n",
        "dbscan = DBSCAN(eps=eps, min_samples=min_samples, metric='euclidean')\n",
        "dbscan.fit(X_reduced)\n",
        "\n",
        "# From Reference [3]\n",
        "# Define function to plot DBSCAN results\n",
        "def plot_dbscan(dbscan, X, size, show_xlabels=True, show_ylabels=True):\n",
        "    core_mask = np.zeros_like(dbscan.labels_, dtype=bool)\n",
        "    core_mask[dbscan.core_sample_indices_] = True\n",
        "    anomalies_mask = dbscan.labels_ == -1\n",
        "    non_core_mask = ~(core_mask | anomalies_mask)\n",
        "\n",
        "    cores = dbscan.components_\n",
        "    anomalies = X[anomalies_mask]\n",
        "    non_cores = X[non_core_mask]\n",
        "\n",
        "    plt.scatter(cores[:, 0], cores[:, 1], c=dbscan.labels_[core_mask], marker='+', s=size, cmap=\"Paired\")\n",
        "    plt.scatter(anomalies[:, 0], anomalies[:, 1], c=\"r\", marker=\"x\", s=100)\n",
        "    plt.scatter(non_cores[:, 0], non_cores[:, 1], c=dbscan.labels_[non_core_mask], marker=\"*\")\n",
        "\n",
        "    if show_xlabels:\n",
        "        plt.xlabel(\"$x_1$\")\n",
        "    if show_ylabels:\n",
        "        plt.ylabel(\"$x_2$\", rotation=0)\n",
        "    plt.title(f\"eps={dbscan.eps:.2f}, min_samples={dbscan.min_samples}\")\n",
        "    plt.grid()\n",
        "    plt.gca().set_axisbelow(True)\n",
        "\n",
        "# Plotting the results for DBSCAN with eps=0.05\n",
        "plt.figure(figsize=(6, 4))\n",
        "plot_dbscan(dbscan, X_reduced, size=100)\n",
        "plt.show()\n",
        "\n",
        "labels = dbscan.labels_\n",
        "n_clusters = len(set(labels)) - (1 if -1 in labels else 0)  # Subtract 1 for noise (-1 label)\n",
        "cluster_counts = Counter(labels)\n",
        "n_noise = list(labels).count(-1)\n",
        "\n",
        "print(f'eps: {eps}')\n",
        "print(f'min_samples: {min_samples}')\n",
        "print(f'Number of clusters: {n_clusters}')\n",
        "print(f'Number of noise points: {n_noise}')\n",
        "print(f\"Number of samples in each cluster:\")\n",
        "for label, count in sorted(cluster_counts.items()):\n",
        "    if label == -1:\n",
        "        print(f\"Noise (label -1): {count} samples\")\n",
        "    else:\n",
        "        print(f\"Cluster {label}: {count} samples\")\n",
        "if n_clusters > 1:\n",
        "    silhouette_avg = silhouette_score(X_reduced, labels)\n",
        "    print(f\"Silhouette Score: {silhouette_avg}\")"
      ],
      "metadata": {
        "id": "NT2ZlI2loxfk"
      },
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "markdown",
      "source": [
        "## References  \n",
        "[1] https://www.kaggle.com/code/serkanpeldek/face-recognition-on-olivetti-dataset  \n",
        "[2] https://e.centennialcollege.ca/d2l/le/content/1186996/viewContent/14778766/View (K-Means lab)  \n",
        "[3] https://e.centennialcollege.ca/d2l/le/content/1186996/viewContent/14778765/View (DBSCAN lab)"
      ],
      "metadata": {
        "id": "yOUfsvlmo7OG"
      }
    },
    {
      "cell_type": "markdown",
      "source": [],
      "metadata": {
        "id": "quFSuD45FL5t"
      }
    }
  ]
}