{
  "nbformat": 4,
  "nbformat_minor": 0,
  "metadata": {
    "colab": {
      "private_outputs": true,
      "provenance": [],
      "authorship_tag": "ABX9TyOa2MLPn4vCvjg50UfO++R9",
      "include_colab_link": true
    },
    "kernelspec": {
      "name": "python3",
      "display_name": "Python 3"
    },
    "language_info": {
      "name": "python"
    }
  },
  "cells": [
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "view-in-github",
        "colab_type": "text"
      },
      "source": [
        "<a href=\"https://colab.research.google.com/github/Ayman850/COMP257/blob/main/COMP257_Ass4_AymanYasin.ipynb\" target=\"_parent\"><img src=\"https://colab.research.google.com/assets/colab-badge.svg\" alt=\"Open In Colab\"/></a>"
      ]
    },
    {
      "cell_type": "markdown",
      "source": [
        "# Assignment4 - Question 1"
      ],
      "metadata": {
        "id": "qoy7afjuBcyd"
      }
    },
    {
      "cell_type": "markdown",
      "source": [
        "## Imports"
      ],
      "metadata": {
        "id": "GJMc8-7fBhIE"
      }
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {
        "id": "fBlXlqnBPfls"
      },
      "outputs": [],
      "source": [
        "#1st I imported all necessaries import for this assignemnt i need, basically for this assignment we are using GaussianMixture.\n",
        "\n",
        "import matplotlib.pyplot as plt\n",
        "import numpy as np\n",
        "import random\n",
        "from sklearn.datasets import fetch_olivetti_faces\n",
        "from sklearn.model_selection import train_test_split\n",
        "from sklearn.decomposition import PCA\n",
        "from sklearn.mixture import GaussianMixture\n",
        "from scipy.ndimage import rotate"
      ]
    },
    {
      "cell_type": "markdown",
      "source": [
        "## 1. Retrieve and load the Olivetti faces dataset"
      ],
      "metadata": {
        "id": "G-nkKSbDBlVd"
      }
    },
    {
      "cell_type": "code",
      "source": [
        "# 1st step is to Retrieve and load the Olivetti faces dataset which is the same as previous assignment (ass2 and ass3)\n",
        "\n",
        "olivetti_faces = fetch_olivetti_faces()\n",
        "X = olivetti_faces.data\n",
        "y = olivetti_faces.target\n",
        "images = olivetti_faces.images # 64x64 version of .data\n",
        "\n",
        "# here we printed the shape to see if its correct\n",
        "\n",
        "print(X.shape)\n",
        "print(y.shape)"
      ],
      "metadata": {
        "id": "bKmVGFPfBoDU"
      },
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "markdown",
      "source": [
        "## 2. Split the dataset into training, validation, and test sets using stratified sampling to ensure that each set contains the same number of images per person."
      ],
      "metadata": {
        "id": "y8yL6bLSBzP4"
      }
    },
    {
      "cell_type": "code",
      "source": [
        "# Here I split the dataset like previous ass ( Split is 80% train, 10% validation, and 10% test)\n",
        "\n",
        "# train has 8 images for 1 person, 1 images for validation, and 1 images for test)\n",
        "\n",
        "'''\n",
        "Split is:\n",
        "80% train\n",
        "10% validation\n",
        "10% test\n",
        "to ensure we have a lot of data to train,\n",
        "and also still have some left for validation and testing\n",
        "'''\n",
        "# First split into train + validation (90%) and test (15%)\n",
        "X_train_val, X_test, y_train_val, y_test = train_test_split(\n",
        "    X, y, test_size=0.1, random_state=42, stratify=y)\n",
        "\n",
        "# Now split the train + validation set (90%) into train (80%) and validation (10%)\n",
        "# 10/90 = 0.1111\n",
        "X_train, X_val, y_train, y_val = train_test_split(\n",
        "    X_train_val, y_train_val, test_size=0.1111, random_state=42, stratify=y_train_val)"
      ],
      "metadata": {
        "id": "0u42j9PiB0Lm"
      },
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "code",
      "source": [
        "# here we print the shape again to see if its correct\n",
        "\n",
        "print(X_train.shape)\n",
        "print(X_val.shape)\n",
        "print(X_test.shape)\n",
        "\n",
        "print(y_train.shape)\n",
        "print(y_val.shape)\n",
        "print(y_test.shape)"
      ],
      "metadata": {
        "id": "GRU15O0zB5q0"
      },
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "markdown",
      "source": [
        "## 3. Apply PCA on the training data, preserving 99% of the variance, to reduce the datasetâ€™s dimensionality."
      ],
      "metadata": {
        "id": "iJMsKlohCRDY"
      }
    },
    {
      "cell_type": "code",
      "source": [
        "# here I used PCA to preserve 99% of the variance, and that is by putting n_components=0.99.\n",
        "\n",
        "pca = PCA(n_components=0.99)\n",
        "\n",
        "X_train_reduced = pca.fit_transform(X_train)"
      ],
      "metadata": {
        "id": "cjeRU9RtCR-M"
      },
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "code",
      "source": [
        "# here we found that it reduced 222 feature from 4096\n",
        "\n",
        "print(X_train_reduced.shape)\n",
        "print(y_train.shape)"
      ],
      "metadata": {
        "id": "RM9W3d3NCYQO"
      },
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "markdown",
      "source": [
        "## 4. Determine the most suitable covariance type for the dataset."
      ],
      "metadata": {
        "id": "vE_UkijNCb6V"
      }
    },
    {
      "cell_type": "code",
      "source": [
        "# now we are trying to find most suitable covariance type for the dataset, and i did this by using AIC and BIC scores, for this i set the n_components = 20, and then i look through all the covariance_types.\n",
        "\n",
        "covariance_types = [\"spherical\", \"diag\", \"tied\", \"full\"]\n",
        "\n",
        "aic_scores_4 = []\n",
        "bic_scores_4 = []\n",
        "n_components = 20\n",
        "n_inits = 10\n",
        "\n",
        "for c_type in covariance_types:\n",
        "    gm = GaussianMixture(\n",
        "        n_components=n_components,\n",
        "        covariance_type=c_type,\n",
        "        n_init=n_inits,\n",
        "        random_state=42)\n",
        "    gm.fit(X_train_reduced)\n",
        "\n",
        "    # Get scores\n",
        "    aic = gm.aic(X_train_reduced)\n",
        "    bic = gm.bic(X_train_reduced)\n",
        "\n",
        "    # Collect scores\n",
        "    aic_scores_4.append(aic)\n",
        "    bic_scores_4.append(bic)\n",
        "\n",
        "    print(f\"Covariance type: {c_type}; AIC: {aic}; BIC: {bic}\")\n",
        "\n",
        "\n",
        "# At the end we find the best covariance type, and i print it, and from results it seems that both this scores \"diag\" is our best covariance type.\n",
        "\n",
        "best_cov_aic = covariance_types[aic_scores_4.index(min(aic_scores_4))]\n",
        "best_cov_bic = covariance_types[bic_scores_4.index(min(bic_scores_4))]\n",
        "\n",
        "print(f\"Best covariance type by AIC: {best_cov_aic}\")\n",
        "print(f\"Best covariance type by BIC: {best_cov_bic}\")"
      ],
      "metadata": {
        "id": "mWPzF2QkCcgz"
      },
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "markdown",
      "source": [
        "## 5. Determine the minimum number of clusters that best represent the dataset using either AIC or BIC."
      ],
      "metadata": {
        "id": "GRBCxj0OCrSe"
      }
    },
    {
      "cell_type": "code",
      "source": [
        "# now as we found our best covariance type which is diag, we used this again to find the minimum number of clusters where I use the range from 1-21, so it's basically I find 1 upto 20 to see which one is the best\n",
        "\n",
        "covariance_type = \"diag\"\n",
        "n_components_range = range(1, 21)\n",
        "n_inits = 10\n",
        "aic_scores_5 = []\n",
        "bic_scores_5 = []\n",
        "\n",
        "# this is the same method again basically\n",
        "\n",
        "for i in n_components_range:\n",
        "    gm = GaussianMixture(\n",
        "        n_components=i,\n",
        "        covariance_type=covariance_type,\n",
        "        n_init=n_inits,\n",
        "        random_state=42)\n",
        "    gm.fit(X_train_reduced)\n",
        "\n",
        "    # Get scores\n",
        "    aic = gm.aic(X_train_reduced)\n",
        "    bic = gm.bic(X_train_reduced)\n",
        "\n",
        "    # Collect scores\n",
        "    aic_scores_5.append(aic)\n",
        "    bic_scores_5.append(bic)\n",
        "\n",
        "    print(f\"# of cluster: {i}; AIC: {aic}; BIC: {bic}\")\n",
        "\n",
        "# Then we find the best (scores) number of cluster\n",
        "\n",
        "best_n_aic = n_components_range[aic_scores_5.index(min(aic_scores_5))]\n",
        "best_n_bic = n_components_range[bic_scores_5.index(min(bic_scores_5))]\n",
        "\n",
        "# Then we print the best number of cluster\n",
        "\n",
        "print(f\"Best # of clusters by AIC: {best_n_aic}\")\n",
        "print(f\"Best # of clusters by BIC: {best_n_bic}\")\n",
        "\n",
        "# here I found the number of clusters is AIC: 3 and, BIC: 2"
      ],
      "metadata": {
        "id": "Qz-mMAU7CsLl"
      },
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "markdown",
      "source": [
        "## 6. Plot the results from steps 3, 4, and 5."
      ],
      "metadata": {
        "id": "KigMOCwlCzJI"
      }
    },
    {
      "cell_type": "code",
      "source": [
        "# Now I Ploting the results from steps 3, 4, and 5. and here steps 3 is for PCA, here we can see the highest point is 222 in order to reach 0.99\n",
        "\n",
        "# Step 3 results\n",
        "cumulative_variance = pca.explained_variance_ratio_.cumsum()\n",
        "max_x = X_train_reduced.shape[1]\n",
        "max_y = cumulative_variance[max_x - 1]\n",
        "\n",
        "plt.figure(figsize=(8, 4))\n",
        "plt.plot(range(1, len(cumulative_variance) + 1), cumulative_variance)\n",
        "plt.xlabel('Number of Components')\n",
        "plt.ylabel('Cumulative Explained Variance')\n",
        "plt.title('Cumulative Explained Variance by PCA Components')\n",
        "\n",
        "# Highlight the highest point\n",
        "plt.plot(max_x, max_y, 'ro')  # 'ro' makes it a red dot\n",
        "plt.text(max_x, max_y, f'({max_x}, {max_y:.2f})', ha='left', va='bottom', color='red')\n",
        "\n",
        "# Add a vertical dotted line\n",
        "plt.axvline(x=max_x, color='red', linestyle='--', linewidth=1)\n",
        "\n",
        "# Set x-axis ticks at intervals of 20\n",
        "plt.xticks(range(0, len(cumulative_variance) + 1, 20))\n",
        "\n",
        "plt.grid(True)\n",
        "plt.tight_layout()\n",
        "plt.show()"
      ],
      "metadata": {
        "id": "s-W81jDtC0CW"
      },
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "code",
      "source": [
        "# next step is the result of when i trying to find the Different Covariance Types using AIC and BIC as we can see the lowest bar for both blue and oragne is diag\n",
        "\n",
        "# Step 4 results\n",
        "plt.figure(figsize=(8, 4))\n",
        "\n",
        "x_axis_4 = np.arange(len(covariance_types))\n",
        "plt.bar(x_axis_4 - 0.2, aic_scores_4, 0.4, label=\"AIC\")\n",
        "plt.bar(x_axis_4 + 0.2, bic_scores_4, 0.4, label=\"BIC\")\n",
        "plt.xticks(x_axis_4, covariance_types)\n",
        "plt.xlabel('Covariance Type')\n",
        "plt.ylabel('AIC/BIC Score')\n",
        "plt.title('AIC/BIC for Different Covariance Types')\n",
        "plt.legend()\n",
        "\n",
        "plt.tight_layout()\n",
        "plt.show()"
      ],
      "metadata": {
        "id": "0_l_uSfgLbTK"
      },
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "code",
      "source": [
        "# for this step we found Numbers of Clusters, and here actually used diag covariance try to find Different Numbers of Clusters ..as you can see the minimum point for oragne is 2 (which is for BIC), and minimum pointfor blue is 3 (which is for AIC)\n",
        "# Step 5 results\n",
        "plt.figure(figsize=(8, 4))\n",
        "\n",
        "plt.plot(n_components_range, aic_scores_5, label='AIC', marker='o')\n",
        "plt.plot(n_components_range, bic_scores_5, label='BIC', marker='o')\n",
        "plt.xticks(n_components_range)\n",
        "plt.xlabel('Number of Clusters')\n",
        "plt.ylabel('AIC/BIC Score')\n",
        "plt.title(f'AIC/BIC for Different Numbers of Clusters (diag covariance)')\n",
        "plt.legend()\n",
        "\n",
        "plt.tight_layout()\n",
        "plt.show()"
      ],
      "metadata": {
        "id": "GM6dXgusLhXO"
      },
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "markdown",
      "source": [
        "## 7. Output the hard clustering assignments for each instance to identify which cluster each image belongs to."
      ],
      "metadata": {
        "id": "laatPWiGLody"
      }
    },
    {
      "cell_type": "code",
      "source": [
        "# Now we try to Output the hard clustering assignments for each instance and we do that by making GaussianMixture variable with this 2 parameters ( n_components = 2 # from BIC, covariance_type = \"diag\") which is the best parameter we found from above.\n",
        "\n",
        "# Using parameters from above\n",
        "n_components = 2 # from BIC\n",
        "covariance_type = \"diag\"\n",
        "n_inits = 10 # here we used same number for inits\n",
        "\n",
        "gm = GaussianMixture(\n",
        "    n_components=n_components,\n",
        "    covariance_type=covariance_type,\n",
        "    n_init=n_inits,\n",
        "    random_state=42)\n",
        "gm.fit(X_train_reduced)\n",
        "\n",
        "y_train_hard = gm.predict(X_train_reduced)\n",
        "print(y_train_hard)\n",
        "\n",
        "counts = np.bincount(y_train_hard)\n",
        "print(f\"Count of 0s: {counts[0]}\")\n",
        "print(f\"Count of 1s: {counts[1]}\")\n",
        "\n",
        "# after we print the result, 169 data points that belong to clusters-0 and 151 belongs to clusters-1\n",
        "\n",
        "#below predecting 120 data point gives me 120 labels"
      ],
      "metadata": {
        "id": "J-RCECmtLpTN"
      },
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "code",
      "source": [
        "# I print the shape to chekh if everything is correct, and the shape is 320\n",
        "y_train_hard.shape"
      ],
      "metadata": {
        "id": "uqmnpJ9uLvfQ"
      },
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "markdown",
      "source": [
        "## 8. Output the soft clustering probabilities for each instance to show the likelihood of each image belonging to each cluster."
      ],
      "metadata": {
        "id": "VDkM8IpULzqD"
      }
    },
    {
      "cell_type": "code",
      "source": [
        "# next I find the soft clustering probabilities and this is because we only have to 2 clusters, it should be 2 coloums where 1st coloum is for cluster 0 and 2nd coloum is for cluster 1.\n",
        "\n",
        "soft_clustering_probabilities = gm.predict_proba(X_train_reduced)\n",
        "\n",
        "# 3 columns, each column is the likelihood of the image being in a cluster\n",
        "print(soft_clustering_probabilities)"
      ],
      "metadata": {
        "id": "xb0aR-8gL0i3"
      },
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "code",
      "source": [
        "#print the shape just to make sure\n",
        "\n",
        "soft_clustering_probabilities.shape"
      ],
      "metadata": {
        "id": "7nCVzlFoMGjB"
      },
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "markdown",
      "source": [
        "## 9. Use the model to generate some new faces (using the sample() method) and visualize them (use the inverse_transform() method to transform the data back to its original space based on the PCA method used)."
      ],
      "metadata": {
        "id": "Aars6YuiMIEE"
      }
    },
    {
      "cell_type": "code",
      "source": [
        "# I used the model to Generated new faces using gm.sample of 5 faces\n",
        "\n",
        "X_new_reduced, y_new_reduced = gm.sample(5)\n",
        "print(\"Generated new points:\\n\", X_new_reduced)\n",
        "print(\"Generated new labels:\\n\", y_new_reduced)"
      ],
      "metadata": {
        "id": "2PWFJELIMLZj"
      },
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "code",
      "source": [
        "X_new_original = pca.inverse_transform(X_new_reduced)\n",
        "\n",
        "fig, axarr = plt.subplots(nrows=1, ncols=5, figsize=(18,9))\n",
        "axarr = axarr.flatten()\n",
        "for i in range(5):\n",
        "    axarr[i].imshow(X_new_original[i].reshape(64, 64), cmap='gray')\n",
        "    axarr[i].set_xticks([])\n",
        "    axarr[i].set_yticks([])\n",
        "    axarr[i].set_title(f\"cluster id: {y_new_reduced[i]}\")"
      ],
      "metadata": {
        "id": "02jpz2a4MVCo"
      },
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "markdown",
      "source": [
        "## 10. Modify some images (e.g., rotate, flip, darken)."
      ],
      "metadata": {
        "id": "YrbwV-HWMZ4B"
      }
    },
    {
      "cell_type": "code",
      "source": [
        "# Next I Modify some images\n",
        "\n",
        "X_modified = []\n",
        "random.seed(42) # for random angles\n",
        "\n",
        "# Modify first 5 images in training set\n",
        "for i in range(5):\n",
        "    original_image = X_train[i].reshape(64, 64)\n",
        "\n",
        "    ## Rotate image (random angles between -180 and 180)\n",
        "    modified_image = rotate(original_image, angle=random.randint(-180, 180), reshape=False)\n",
        "\n",
        "    ##Flip horizontally\n",
        "    modified_image = np.flip(modified_image, axis=1)\n",
        "\n",
        "    ## Darken\n",
        "    modified_image = modified_image - 0.45\n",
        "\n",
        "    ## Ensure values are still between 0 and 1\n",
        "    modified_image = np.clip(modified_image, 0, 1)\n",
        "\n",
        "    X_modified.append(modified_image)\n",
        "\n",
        "# Plot unmodified vs modified\n",
        "fig, axarr = plt.subplots(nrows=2, ncols=5, figsize=(18,9))\n",
        "axarr = axarr.flatten()\n",
        "for i in range(5):\n",
        "    axarr[i].imshow(X_train[i].reshape(64, 64), cmap='gray')\n",
        "    axarr[i].set_xticks([])\n",
        "    axarr[i].set_yticks([])\n",
        "    axarr[i].set_title(f\"person id: {y_train[i]}\")\n",
        "for i in range(5, 10):\n",
        "    axarr[i].imshow(X_modified[i-5], cmap='gray')\n",
        "    axarr[i].set_xticks([])\n",
        "    axarr[i].set_yticks([])\n",
        "    axarr[i].set_title(f\"person id: {y_train[i-5]}\")\n",
        "plt.suptitle(\"Original vs Modified\")\n",
        "plt.tight_layout()\n",
        "plt.show()\n",
        "\n",
        "# here i plot 1st 5 images from train images\n",
        "# we can see that person id:31 image, which after modification we can see on 2nd row"
      ],
      "metadata": {
        "id": "GReVFFd4Ma_C"
      },
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "markdown",
      "source": [
        "## 11. Determine if the model can detect the anomalies produced in step 10 by comparing the output of the score_samples() method for normal images and for anomalies."
      ],
      "metadata": {
        "id": "woKidsPMMkvv"
      }
    },
    {
      "cell_type": "code",
      "source": [
        "# Last we check if we can detect the anomalies\n",
        "# Initialize variables\n",
        "X_normals = X_train[:5]\n",
        "X_anomalies = np.array(X_modified).reshape((5, 4096))\n",
        "print(X_normals.shape)\n",
        "print(X_anomalies.shape)"
      ],
      "metadata": {
        "id": "2DOLTEXWMlgS"
      },
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "code",
      "source": [
        "# Apply PCA ( we used same PCA as above ) so it fits the model\n",
        "X_normals_reduced = pca.transform(X_normals)\n",
        "X_anomalies_reduced = pca.transform(X_anomalies)\n",
        "print(X_normals_reduced.shape)\n",
        "print(X_anomalies_reduced.shape)"
      ],
      "metadata": {
        "id": "DvDJRQ8PMuF8"
      },
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "code",
      "source": [
        "# Get scores\n",
        "normal_scores = gm.score_samples(X_normals_reduced)\n",
        "anomaly_scores = gm.score_samples(X_anomalies_reduced)"
      ],
      "metadata": {
        "id": "QUSH6hy9Mz06"
      },
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "code",
      "source": [
        "normal_scores"
      ],
      "metadata": {
        "id": "SbCoVCE9M25L"
      },
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "code",
      "source": [
        "anomaly_scores"
      ],
      "metadata": {
        "id": "rOA3xwvAM-hI"
      },
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "markdown",
      "source": [
        "- After we print the normal_scores and anomaly_scores we can see both scores are very diffrent and anomaly_scores is much lower then the normal_scores\n",
        "\n",
        "\n",
        "- I belive it seems to be able to detect the anomalies. the scores of the modified images are much lower compared to the images that are unmodified.\n",
        "\n",
        "> Add blockquote\n",
        "\n"
      ],
      "metadata": {
        "id": "paMnw6DHNMQL"
      }
    }
  ]
}