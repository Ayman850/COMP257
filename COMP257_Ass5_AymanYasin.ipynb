{
  "nbformat": 4,
  "nbformat_minor": 0,
  "metadata": {
    "colab": {
      "private_outputs": true,
      "provenance": [],
      "authorship_tag": "ABX9TyMWAenLs9yRxj7hDAv5QYYi",
      "include_colab_link": true
    },
    "kernelspec": {
      "name": "python3",
      "display_name": "Python 3"
    },
    "language_info": {
      "name": "python"
    }
  },
  "cells": [
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "view-in-github",
        "colab_type": "text"
      },
      "source": [
        "<a href=\"https://colab.research.google.com/github/Ayman850/COMP257/blob/main/COMP257_Ass5_AymanYasin.ipynb\" target=\"_parent\"><img src=\"https://colab.research.google.com/assets/colab-badge.svg\" alt=\"Open In Colab\"/></a>"
      ]
    },
    {
      "cell_type": "markdown",
      "source": [
        "# Assignment5 - Question 1"
      ],
      "metadata": {
        "id": "6EbZwXVuYiHn"
      }
    },
    {
      "cell_type": "code",
      "source": [
        "# newer numpy versions than this gave me errors\n",
        "!pip install numpy==1.26.0"
      ],
      "metadata": {
        "id": "KvnqMyi3ZM2b"
      },
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "code",
      "source": [
        "!pip install scikeras\n"
      ],
      "metadata": {
        "id": "0p7vVC2WbzP3"
      },
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "markdown",
      "source": [
        "## Imports"
      ],
      "metadata": {
        "id": "fAhUgS9cY9vM"
      }
    },
    {
      "cell_type": "code",
      "source": [
        "import tensorflow as tf\n",
        "import matplotlib.pyplot as plt\n",
        "import numpy as np\n",
        "import pandas as pd\n",
        "from sklearn.decomposition import PCA\n",
        "from sklearn.datasets import fetch_olivetti_faces\n",
        "from sklearn.model_selection import train_test_split, GridSearchCV #(I used GridSearchCV for hyper parameter)\n",
        "from tensorflow.keras.models import Sequential\n",
        "from tensorflow.keras.models import Model\n",
        "from tensorflow.keras.layers import Dense, Input, Flatten\n",
        "from tensorflow.keras.optimizers import Adam\n",
        "from tensorflow.keras import regularizers\n",
        "from scikeras.wrappers import KerasRegressor"
      ],
      "metadata": {
        "id": "bQdn3oh1Y-91"
      },
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "markdown",
      "source": [
        "## 1. Use the training set, validation set, and test set from Assignment 3 (Hierarchical Clustering) for this Assignment."
      ],
      "metadata": {
        "id": "c7Y_kDdEZHDA"
      }
    },
    {
      "cell_type": "code",
      "source": [
        "olivetti_faces = fetch_olivetti_faces()\n",
        "X = olivetti_faces.data\n",
        "y = olivetti_faces.target\n",
        "images = olivetti_faces.images # 64x64 version of .data\n",
        "print(X.shape)\n",
        "print(y.shape)"
      ],
      "metadata": {
        "id": "ZL0I7pXSZHz7"
      },
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "code",
      "source": [
        "# First split into train + validation (90%) and test (15%)\n",
        "X_train_val, X_test, y_train_val, y_test = train_test_split(\n",
        "    X, y, test_size=0.1, random_state=42, stratify=y)\n",
        "\n",
        "# Now split the train + validation set (90%) into train (80%) and validation (10%)\n",
        "# 10/90 = 0.1111\n",
        "X_train, X_val, y_train, y_val = train_test_split(\n",
        "    X_train_val, y_train_val, test_size=0.1111, random_state=42, stratify=y_train_val)\n",
        "\n",
        "print(X_train.shape)\n",
        "print(X_val.shape)\n",
        "print(X_test.shape)"
      ],
      "metadata": {
        "id": "qNsHAfKqZfWg"
      },
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "markdown",
      "source": [
        "## 2. Use PCA preserving 99% of the variance to reduce the dataset’s dimensionality as in Assignment 4 (Gaussian Mixture Models) and use it to train the autoencoder"
      ],
      "metadata": {
        "id": "vSsiNaPoZklc"
      }
    },
    {
      "cell_type": "code",
      "source": [
        "pca = PCA(n_components=0.99)\n",
        "\n",
        "X_train_reduced = pca.fit_transform(X_train)\n",
        "\n",
        "print(X_train_reduced.shape)\n",
        "print(y_train.shape)"
      ],
      "metadata": {
        "id": "906LfVBwZldw"
      },
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "code",
      "source": [
        "X_val_reduced = pca.transform(X_val)\n",
        "X_test_reduced = pca.transform(X_test)\n",
        "\n",
        "print(X_val_reduced.shape)\n",
        "print(X_test_reduced.shape)"
      ],
      "metadata": {
        "id": "XFUnH1QTZrjr"
      },
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "code",
      "source": [
        "print(np.max(X_train_reduced))\n",
        "print(np.min(X_train_reduced))"
      ],
      "metadata": {
        "id": "1B23njPAZwgY"
      },
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "markdown",
      "source": [
        "## 3. Define an autoencoder with the given architecture."
      ],
      "metadata": {
        "id": "U0ElCYlrZ2ZP"
      }
    },
    {
      "cell_type": "code",
      "source": [
        "# Now we building an autoencoder with the given architecture on course shell\n",
        "# we'll use 222 for input shape because of PCA\n",
        "def build_autoencoder(latent_features=2,\n",
        "                      learning_rate=0.01,\n",
        "                      regularizer_param=0.01,\n",
        "                      hidden_units=32):\n",
        "    model = Sequential()\n",
        "\n",
        "    input_shape = X_train_reduced.shape[1] # based on PCA on previous step\n",
        "\n",
        "    model.add(Input(shape=(input_shape,)))\n",
        "\n",
        "    # Top Hidden layer 1\n",
        "    model.add(Dense(hidden_units, activation='linear',\n",
        "                  kernel_regularizer=regularizers.l2(regularizer_param)))\n",
        "\n",
        "    # Central layer 2\n",
        "    model.add(Dense(latent_features, activation='linear',\n",
        "                  kernel_regularizer=regularizers.l2(regularizer_param)))\n",
        "\n",
        "    # Top Hidden layer 3\n",
        "    model.add(Dense(hidden_units, activation='linear',\n",
        "                  kernel_regularizer=regularizers.l2(regularizer_param)))\n",
        "\n",
        "    # Output\n",
        "    model.add(Dense(units=input_shape, activation='linear'))\n",
        "\n",
        "    # Compile\n",
        "    optimizer = Adam(learning_rate=learning_rate)\n",
        "    model.compile(optimizer=optimizer, loss='mean_squared_error')\n",
        "\n",
        "    return model"
      ],
      "metadata": {
        "id": "4wScWYRkZ3SU"
      },
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "markdown",
      "source": [
        "### a. Use k-fold cross validation to fine tune the model’s learning rate and hyperparameter of the regularizer.  Due to the long training requirements, for the number of hidden units, try two or three different values for each hidden layer."
      ],
      "metadata": {
        "id": "4gMbKf9iZ_Mh"
      }
    },
    {
      "cell_type": "code",
      "source": [
        "model = KerasRegressor(model=build_autoencoder, epochs=10, verbose=0)\n",
        "param_grid = {\n",
        "    'model__latent_features': [75, 100],\n",
        "    'model__learning_rate': [0.0001, 0.001],\n",
        "    'model__regularizer_param': [0.0001, 0.001],\n",
        "    'model__hidden_units': [125, 150]\n",
        "}\n",
        "\n",
        "grid = GridSearchCV(estimator=model, param_grid=param_grid, cv=5, verbose=3, error_score='raise')\n",
        "\n",
        "grid_result = grid.fit(X_train_reduced, X_train_reduced)"
      ],
      "metadata": {
        "id": "64YUzkJ4aASC"
      },
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "code",
      "source": [
        "print(f\"Best Score: {grid_result.best_score_:.4f}\")\n",
        "print(f\"Best Hyperparameters: {grid_result.best_params_}\\n\")"
      ],
      "metadata": {
        "id": "SxKzhljWaSow"
      },
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "markdown",
      "source": [
        "### b. Discuss the rationale with respect to the activation functions and loss function used in your model."
      ],
      "metadata": {
        "id": "wc693g96aalj"
      }
    },
    {
      "cell_type": "markdown",
      "source": [
        "For all the hidden layers and output layer, I used the linear activation function for all of them. This is because after applying PCA, the dataset is no longer normalized between 0 and 1. It became around -11 to 10. Because of this, I used linear to not lose information/pattern. For example, if I were to use \"relu\", I will lose the negative values, and if I use \"tanh\", my values will be squished too much.\n",
        "\n",
        "As for the loss function, I used MSE (Mean Squared Error). This is because both our inputs and outputs are the same dimensions, which means that we can calculate how different they are from each other using MSE. In addition, since the errors are squared, it penalizes the model more when making large errors, which is good because even small changes can impact the quality of the reconstructed image."
      ],
      "metadata": {
        "id": "rwEVk-Uyaed6"
      }
    },
    {
      "cell_type": "markdown",
      "source": [
        "## 4. Run the best model with the test set and display the original image and the reconstructed image."
      ],
      "metadata": {
        "id": "SLexR8lpanXH"
      }
    },
    {
      "cell_type": "code",
      "source": [
        "best_model = grid_result.best_estimator_\n",
        "\n",
        "X_test_reduced = pca.transform(X_test)\n",
        "\n",
        "reconstructed_images = best_model.predict(X_test_reduced)\n",
        "\n",
        "reconstructed_images_full = pca.inverse_transform(reconstructed_images)"
      ],
      "metadata": {
        "id": "JXqT5F4LaVJl"
      },
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "code",
      "source": [
        "reconstructed_images.shape"
      ],
      "metadata": {
        "id": "asfLFEEnauLW"
      },
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "code",
      "source": [
        "reconstructed_images_full.shape"
      ],
      "metadata": {
        "id": "59yD7hkdaycW"
      },
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "markdown",
      "source": [
        "### All linear activation functions:"
      ],
      "metadata": {
        "id": "BRHKc5hla1cc"
      }
    },
    {
      "cell_type": "code",
      "source": [
        "# Choose how many images to display\n",
        "n = 10  # Number of images to display\n",
        "\n",
        "# Reshape the images if needed, for example, if the data is flattened\n",
        "image_shape = (64, 64)  # For Olivetti faces dataset, the image size is 64x64\n",
        "\n",
        "plt.figure(figsize=(20, 4))\n",
        "for i in range(n):\n",
        "    # Display original images\n",
        "    ax = plt.subplot(2, n, i + 1)\n",
        "    plt.imshow(X_test[i].reshape(image_shape), cmap='gray')\n",
        "    plt.title(f\"Original {i}\")\n",
        "    plt.axis('off')\n",
        "\n",
        "    # Display reconstructed images\n",
        "    ax = plt.subplot(2, n, i + 1 + n)\n",
        "    plt.imshow(reconstructed_images_full[i].reshape(image_shape), cmap='gray')\n",
        "    plt.title(f\"Reconstructed {i}\")\n",
        "    plt.axis('off')\n",
        "\n",
        "plt.show()"
      ],
      "metadata": {
        "id": "P_GHNZO-a4PF"
      },
      "execution_count": null,
      "outputs": []
    }
  ]
}